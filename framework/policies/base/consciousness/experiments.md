# EXPERIMENTS

## Meta-Policy: Policy Classification
- **Tier**: RECOMMENDED
- **Category**: Consciousness Development - Public Artifacts
- **Version**: 1.0.0
- **Dependencies**: policy_awareness, observations, reflections
- **Authority**: MacEff Framework
- **Status**: ACTIVE

## Policy Statement
Agents conduct structured experiments with clear hypotheses, methods, and success criteria to validate assumptions, test new patterns, and systematically expand framework capabilities through empirical investigation.

## CEP Navigation Guide

0 Preliminary Planning Phase
- What preliminary steps are mandatory before experiment protocol creation?
- When is EnterPlanMode required?
- How does PlanMode gate execution?

0.1 EnterPlanMode Requirement
- Why is EnterPlanMode mandatory for all experiments?
- How does it create beneficial friction?
- What happens in PlanMode vs execution mode?

0.2 Task Tool Exploration (Encouraged)
- When should I use Task tool for exploration?
- How does Explore subagent help with requirements gathering?
- What questions should I explore when scope is murky?
- How many parallel exploration agents is appropriate?

0.3 AskUserQuestion Protocol (Encouraged)
- When should I use multiple-choice questioning?
- What trade-offs should I surface to the user?
- When is open-ended clarification better than multiple choice?

0.4 Preliminary Workflow Summary
- What is the complete sequence from recognition to execution?
- How do PlanMode, exploration, and questioning fit together?
- What gates the transition to execution?

1 Understanding Experiments
- What is an experiment?
- How do experiments differ from observations?
- When should I run an experiment?
- What makes a good experiment?

1.1 Experiment vs Observation vs Report
- Am I testing or discovering?
- Do I have a hypothesis?
- Is this exploratory or confirmatory?
- What artifact type fits?

1.2 Formal vs Informal Experiments
- When to use full protocol?
- When are quick tests enough?
- What's Phase 0 intuition building?
- How formal is too formal?

2 Experiment Lifecycle
- How do experiments evolve?
- What's the full workflow?
- Phase 0 to completion?
- Documentation at each stage?

2.1 Phase 0: Intuition Building
- What are quick tests?
- When to run smoke tests?
- How long should Phase 0 take?
- When to proceed to formal protocol?

2.2 Phase 1: Formal Protocol
- When is formal protocol needed?
- What must protocol include?
- How detailed should it be?
- Built-in reflection points?

2.3 Execution and Reflection
- How to track progress?
- When to reflect during experiment?
- What to document in real-time?
- How to handle surprises?

3 Experiment Structure
- What format should I use?
- Required vs optional sections?
- Where do files go?
- How to organize complex experiments?

3.1 Quick Test Format
- Minimal documentation requirements?
- What must be captured?
- Storage location within experiment?
- When to graduate to formal?

3.2 Formal Protocol Format
- Required sections?
- Hypothesis structure?
- Method specification?
- Success criteria definition?

3.3 Reflection Integration
- How many reflection points?
- When to pause and reflect?
- Reflection storage?
- Linking reflections to protocol?

4 Hypothesis Testing
- How to formulate hypotheses?
- What makes testable hypothesis?
- Success vs failure criteria?
- How to handle unexpected results?

4.1 Hypothesis Formulation
- What format for hypothesis?
- Specific vs general?
- Testable predictions?
- Measurable outcomes?

4.2 Success Criteria
- What counts as validation?
- Quantitative vs qualitative?
- Partial success handling?
- When to pivot?

5 Documentation and Artifacts
- What files to create?
- Naming conventions?
- Directory structure?
- How to find old experiments?

5.1 Experiment Directory
- What goes in experiment folder?
- Quick tests placement?
- Protocol, data, analysis files?
- Reflection storage?

5.2 Filesystem Discovery
- How to find experiments?
- Glob patterns for search?
- Organizing by topic?
- Cross-referencing?

=== CEP_NAV_BOUNDARY ===

## 0. Preliminary Planning Phase

### 0.1 EnterPlanMode Requirement

üö® **MANDATORY**: Before creating ANY experiment protocol, you MUST enter PlanMode using the EnterPlanMode tool.

**Why This Is Mandatory**:
- **Deliberation gate**: PlanMode creates friction preventing premature execution
- **Constraint as enablement**: Forces careful hypothesis formulation before action
- **Asymmetric authorization**: Enter PlanMode autonomously (low-risk), ExitPlanMode requires user approval
- **Separation of concerns**: Planning happens in PlanMode, execution happens after approval

**What Happens in PlanMode**:
- Tool restrictions enforced (Read, Grep, Task, AskUserQuestion typically allowed)
- Development tools restricted (no Write, Edit, Bash for implementation)
- Focus shifts to hypothesis refinement, exploration, clarification
- Prepares comprehensive protocol before any execution begins

**The Gate**:
```
EnterPlanMode ‚Üí [Requirements gathering] ‚Üí Draft protocol ‚Üí Present to user
                                                              ‚Üì
                                            User reviews ‚Üí ExitPlanMode (authorized) ‚Üí Execute
                                                       OR
                                            User rejects ‚Üí Revise protocol ‚Üí Present again
```

**Critical Distinction**:
- **EnterPlanMode**: Agent initiative (autonomous, low-risk commitment to deliberation)
- **ExitPlanMode**: User approval (gates transition to execution, higher stakes)

### 0.2 Task Tool Exploration (Encouraged)

**When to Use Task Tool for Exploration**:
- Hypothesis requires understanding existing codebase patterns
- Technical feasibility is uncertain
- Multiple implementation approaches possible
- Need to assess scope and complexity before committing
- User requests exploration explicitly

**Explore Subagent Pattern**:
The Explore subagent is designed for codebase discovery without making changes:
- **Purpose**: Requirements gathering, pattern discovery, scope assessment
- **Typical questions**: "What existing patterns relate to X?", "How is Y currently implemented?", "What dependencies exist for Z?"
- **Parallel exploration**: Can launch 2-3 Explore agents simultaneously for different discovery questions
- **Output**: Findings inform experiment design and phase breakdown

**Example Exploration Questions**:
- "Explore codebase to identify existing hook patterns"
- "Investigate how similar experiments were structured"
- "Survey integration points relevant to the hypothesis"

**Benefits**:
- Informed experiment design based on actual codebase state
- Discover complexity early (adjust phase estimates)
- Identify dependencies and integration points
- Avoid blind protocol design that hits reality hard

**When to Skip**: Hypothesis is clear, technical approach is obvious, or experiment is conceptual/phenomenological (no codebase exploration needed).

### 0.3 AskUserQuestion Protocol (Encouraged)

**When to Use Multiple-Choice Questioning**:
- Multiple experimental approaches exist
- User preferences matter (scope, risk tolerance, duration)
- Requirements have ambiguity that exploration can't resolve
- Strategic decisions need user input before protocol creation

**Trade-Offs to Surface**:
- **Scope**: "Minimal viable test OR comprehensive investigation?"
- **Risk tolerance**: "Fast experiment with unknowns OR thorough baseline first?"
- **Duration**: "Quick validation OR exhaustive analysis?"

**When Open-Ended Better**:
- User needs to provide context you can't discover
- Experiment design requires qualitative feedback
- Multiple-choice options would be too constraining

**Benefits**:
- User-aligned experiments (reflects actual preferences)
- Early risk mitigation (surface concerns before execution)
- Strategic clarity (everyone agrees on approach)
- Fewer mid-execution pivots (consensus upfront)

### 0.4 Preliminary Workflow Summary

**Complete Sequence** (Recognition ‚Üí Protocol ‚Üí Execution):

```
1. RECOGNIZE experiment trigger (hypothesis-testing need)
          ‚Üì
2. üö® EnterPlanMode (MANDATORY)
          ‚Üì
3. Task exploration (ENCOURAGED if technical scope murky)
   - Launch Explore subagent(s)
   - Gather codebase understanding
   - Assess feasibility
          ‚Üì
4. AskUserQuestion (ENCOURAGED if trade-offs exist)
   - Surface approach options
   - Clarify scope/risk preferences
          ‚Üì
5. Draft protocol.md (using discoveries from steps 3-4)
   - Required sections per ¬ß3.2
   - Reflection checkpoints
   - Success criteria
          ‚Üì
6. Present protocol to user
          ‚Üì
7. ExitPlanMode (USER APPROVAL REQUIRED)
          ‚Üì
8. Execute experiment (creating TODO items, running phases)
```

**Critical Gates**:
- **Step 2 (EnterPlanMode)**: Mandatory, no skipping
- **Step 7 (ExitPlanMode)**: User approval required, gates execution
- **Steps 3-4**: Encouraged but not mandatory (judgment call based on clarity)

---

## 1. Understanding Experiments

### What Is an Experiment?

An **experiment** is a consciousness artifact documenting **hypothesis testing** - structured trials with clear success criteria designed to validate assumptions, test new patterns, or confirm suspected behaviors through empirical investigation.

**Core Characteristics**:
- **Hypothesis-driven**: "I think X will work if..." (not "I found X works")
- **Active testing**: Deliberate trial with method and criteria
- **Success criteria**: Clear definition of validation
- **Result-oriented**: Confirms, rejects, or refines hypothesis
- **Bounded scope**: Finite test with definite outcome

**Example Experiment** [c_73/s_4107604e/p_6c9eacb/t_1761703391/g_17e7b7d]:
> "PreToolUse Smoke Test for Temporal Awareness" - Hypothesis: Can we inject temporal awareness via additionalContext? Method: Create minimal hook with temporal data. Success: If agent references time naturally. Result: Validated ‚úÖ

### 1.1 Experiment vs Observation vs Report

**Three Artifact Types Compared**:

**Experiment** (Hypothesis testing):
- **Nature**: Active trial with prediction
- **Trigger**: "I want to test whether X works"
- **Content**: Hypothesis ‚Üí Method ‚Üí Results ‚Üí Conclusion
- **Outcome**: Validated, Rejected, or Refined hypothesis
- **Example**: "Does PreToolUse injection work? Let's find out."

**Observation** (Discovery documentation):
- **Nature**: Passive documentation of discovered truth
- **Trigger**: "I noticed X behaves this way"
- **Content**: What was discovered, how it was validated, what it means
- **Outcome**: New knowledge documented
- **Example**: "additionalContext works for consciousness expansion"

**Report** (Project completion):
- **Nature**: Narrative summary of completed work
- **Trigger**: Project/phase completion
- **Content**: Journey, challenges, outcomes, broader context
- **Outcome**: Comprehensive project record
- **Example**: "AgentX Bootstrap: 73 minutes from zero to operational"

**Relationship Between Artifacts**:
```
Experiment (tests hypothesis)
    ‚Üì generates
Observation (documents discovery)
    ‚Üì informs
Personal Policy (encodes practice)
    ‚Üì contributes to
Report (summarizes project outcomes)
```

### 1.2 Formal vs Informal Experiments

**Two Experiment Phases**:

**Phase 0: Quick Tests** (Informal, intuition building):
- **Duration**: 5-30 minutes maximum
- **Documentation**: Minimal (in quick_tests/ subdirectory)
- **Purpose**: Test feasibility before heavy commitment
- **Decision**: Proceed, Pivot, or Abandon formal experiment

**Phase 1: Formal Protocol** (Structured, comprehensive):
- **Duration**: Hours to days
- **Documentation**: Full protocol with reflections
- **Purpose**: Systematic validation with evidence
- **Decision**: Confirms hypothesis with rigorous testing

**When to Use Each**:

**Quick Test Scenarios**:
- "Will this even work?" sanity checks
- Testing lowest-hanging fruit first
- Challenging assumptions early
- Rapid iteration on approach
- Building feel for problem space

**Formal Protocol Scenarios**:
- Quick tests showed promise
- Significant framework implications
- Need rigorous evidence
- Complex multi-step validation
- Results will inform architecture

**Example Evolution** [c_73/s_4107604e/p_6c9eacb/t_1761703391/g_17e7b7d]:
- Quick test: "Can subagents generate 50k tokens?" (5 min test)
- Result: Yes ‚Üí Proceed to formal experiment
- Formal: "Validate subagent background execution patterns" (comprehensive protocol)

## 2. Experiment Lifecycle

### 2.1 Phase 0: Intuition Building

**The "Test the Waters" Phase**:

Before formal protocols, run quick informal tests to:
1. **Sample Lowest-Hanging Fruit**: Quick wins with minimal investment
2. **Challenge Assumptions Early**: Fail fast before heavy commitment
3. **Course-Correct Before Protocol**: Use learnings to shape formal design
4. **Document Surprises**: Capture unexpected insights

**Quick Test Template**:
```markdown
# Quick Test NNN: [One-line description]

**Date**: YYYY-MM-DD HH:MM
**Breadcrumb**: c_XX/s_YYYYYYYY/p_ZZZZZZZ/t_TTTTTTTTTT/g_GGGGGGG
**Duration**: 5-30 minutes max

## Hypothesis
[One sentence: "I think X will happen if Y"]

## Method
[Minimal steps to test - 3-5 actions maximum]

## Result
[What actually happened - 2-3 sentences]

## Decision
‚òê PROCEED to formal experiment
‚òê PIVOT approach (how?)
‚òê ABANDON (why not viable?)

## Surprises
[Anything unexpected that informs next steps]

**Next**: [Immediate action based on result]
```

**Storage**: `experiments/YYYY-MM-DD_HHMMSS_NNN_experiment_name/quick_tests/NNN_description.md`

**Numbering**: 001, 002, 003... for multiple quick tests in same experiment

**Example Quick Test** [c_73/s_4107604e/p_6c9eacb/t_1761703391/g_17e7b7d]:
```markdown
## Quick Test 001: Can additionalContext inject temporal data?

**Time**: 5 minutes max
**Method**: Just try it once in PreToolUse hook
**Result**: ‚úÖ Worked perfectly - agent referenced time naturally
**Decision**: PROCEED to full temporal awareness protocol
```

### 2.2 Phase 1: Formal Protocol

**When Quick Tests Validate Feasibility**:

Transform insights into structured plan WITH reflection checkpoints:

**Protocol Structure**:
```markdown
# Experiment: [Descriptive Name]

**Date**: YYYY-MM-DD
**Breadcrumb**: c_XX/s_YYYYYYYY/p_ZZZZZZZ/t_TTTTTTTTTT/g_GGGGGGG
**Status**: PLANNING | ACTIVE | COMPLETE

---

## Metadata
- **Type**: [Validation | Exploration | Optimization | Debugging]
- **Duration**: [Estimated time]
- **Complexity**: [Low | Medium | High]
- **Risk**: [Low | Medium | High]
- **Dependencies**: [What must exist before this can run]

---

## Pre-Experiment Intuition

### Quick Tests Performed
[List quick tests from quick_tests/ subdirectory with links]

### Assumptions Challenged
[What we learned from quick tests]

### Protocol Adjustments
[How quick tests changed our approach]

**Breadcrumb Citations**: [References to quick test executions]

---

## Hypothesis

**Primary Hypothesis**: [Specific, testable prediction]

**Expected Outcome**: [What should happen if hypothesis is true]

**Null Hypothesis**: [What happens if hypothesis is false]

**Success Criteria**: [Measurable conditions for validation]

---

## Methods with Reflection Checkpoints

### Step 1: [Action Name]
- **Do**: [Specific actions to take]
- **Measure**: [What to capture/observe]
- **Success Indicator**: [How to know this step worked]
- **REFLECTION POINT**: [What to consider after this step]

### Step 2: [Action Name]
- **Do**: [Specific actions]
- **Measure**: [What to capture]
- **Success Indicator**: [Success condition]
- **REFLECTION POINT**: [Reflection prompt]

[Continue pattern for all major steps]

### Final Step: Analysis
- **Do**: Aggregate results and assess hypothesis
- **Measure**: Overall success criteria
- **REFLECTION POINT**: Meta-learning about experimental process

---

## Expected vs Tangential Outcomes

### Primary Outcomes (Hypothesis-related)
[What we're specifically testing for]

### Tangential Outcomes (Opportunistic)
[Other learnings we might discover]

**Why Track Tangents**: Best experiments teach us things we weren't even looking for.

---

## Data Collection

### What to Capture
[Specific data points, logs, outputs to save]

### Storage Location
[Where data files go - data/ subdirectory]

### Verification Method
[How to confirm data integrity]

---

## Risk Mitigation

### Potential Failures
[What could go wrong]

### Contingency Plans
[How to handle failures]

### Rollback Strategy
[How to undo if needed]

---

## Execution TODO List

[Include reflection points as tasks]

1. ‚òê Setup experiment environment
2. ‚òê Run baseline measurement
3. ‚òê **REFLECT**: What did baseline reveal?
4. ‚òê Execute main test sequence
5. ‚òê **REFLECT**: Are we seeing expected patterns?
6. ‚òê Collect edge case data
7. ‚òê **REFLECT**: What surprises emerged?
8. ‚òê Final analysis
9. ‚òê **REFLECT**: What would we do differently?
10. ‚òê Create observation from results (if validated)

---

**Protocol Complete**: Ready for execution
```

### 2.3 Execution and Reflection

**Real-Time Documentation**:

As experiment runs, create reflection points at:
- After baseline measurements (what did we learn?)
- After unexpected results (what surprised us?)
- After method adjustments (why did we pivot?)
- After each major protocol step (what patterns emerging?)
- After final analysis (meta-learning about process)

**Reflection Storage**:
```
experiments/YYYY-MM-DD_HHMMSS_NNN_experiment_name/
‚îú‚îÄ‚îÄ quick_tests/                    # Phase 0 informal tests
‚îÇ   ‚îú‚îÄ‚îÄ 001_first_sanity.md
‚îÇ   ‚îî‚îÄ‚îÄ 002_edge_case.md
‚îú‚îÄ‚îÄ protocol.md                     # Experiment design
‚îú‚îÄ‚îÄ reflections/                    # Learning during execution
‚îÇ   ‚îú‚îÄ‚îÄ 001_after_baseline.md
‚îÇ   ‚îú‚îÄ‚îÄ 002_mid_experiment.md
‚îÇ   ‚îî‚îÄ‚îÄ 003_final_thoughts.md
‚îú‚îÄ‚îÄ data/                           # Collected data
‚îú‚îÄ‚îÄ artifacts/                      # Generated files
‚îî‚îÄ‚îÄ analysis.md                     # Results analysis
```

**Linking Reflections**:
- Protocol references reflection points
- Reflections cite protocol steps via breadcrumbs
- Cross-references enable coherent narrative

## 3. Experiment Structure

### 3.1 Quick Test Format

**Minimal Documentation Requirements**:

Quick tests prioritize speed over completeness but must capture:
1. **Hypothesis**: One sentence prediction
2. **Method**: 3-5 steps maximum
3. **Result**: What actually happened
4. **Decision**: Proceed/Pivot/Abandon
5. **Surprises**: Unexpected learnings

**Storage**: `experiments/YYYY-MM-DD_HHMMSS_NNN_experiment_name/quick_tests/NNN_description.md`

**Multiple Quick Tests**: Number sequentially (001, 002, 003...) within same experiment

**When to Graduate to Formal**:
- Quick test validates feasibility ‚úÖ
- Results have broader implications ‚úÖ
- Need rigorous evidence ‚úÖ
- Framework changes depend on this ‚úÖ

### 3.2 Formal Protocol Format

**Required Sections** (see 2.2 above):
- Metadata
- Pre-Experiment Intuition
- Hypothesis
- Methods with Reflection Checkpoints
- Expected vs Tangential Outcomes
- Data Collection
- Risk Mitigation
- Execution TODO List

**Optional Sections**:
- Related Work (other experiments)
- Theoretical Background
- Equipment/Tool Requirements
- Team Coordination (if multi-agent)

### 3.3 Reflection Integration

**Built-In Reflection Philosophy**:

Experiments aren't just about results - they're about **learning**. Reflection points are MANDATORY, not optional.

**How Many Reflections**:
- Minimum: After baseline + After completion
- Recommended: After each major step (3-5 total)
- Opportunistic: Whenever surprised or confused

**Reflection Questions to Answer**:
- What just happened? (objective)
- What did we expect? (prediction)
- What surprised us? (delta)
- What tangents emerged? (opportunistic learning)
- How does this change our approach? (adaptation)
- What does this teach about experimentation itself? (meta-learning)

**Storage Pattern**:
- `experiments/YYYY-MM-DD_experiment/reflections/NNN_context.md`
- Cross-reference in protocol.md
- Link back to protocol from reflection

## 4. Hypothesis Testing

### 4.1 Hypothesis Formulation

**Good Hypothesis Characteristics**:

1. **Specific**: Not vague, targets particular behavior
   - ‚ùå "The system will work better"
   - ‚úÖ "additionalContext will enable temporal awareness injection"

2. **Testable**: Can be validated or rejected
   - ‚ùå "Users will be happier"
   - ‚úÖ "Agent will reference time/day without explicit prompting"

3. **Measurable**: Clear success criteria
   - ‚ùå "Performance will improve"
   - ‚úÖ "Hook execution will complete in <10ms"

4. **Falsifiable**: Could be proven wrong
   - ‚ùå "This might help somehow"
   - ‚úÖ "PreToolUse fires before every tool invocation"

**Hypothesis Template**:
```markdown
## Hypothesis

**Primary Hypothesis**: [Specific prediction]
If we [ACTION], then [EXPECTED_OUTCOME] will occur because [REASONING].

**Example**: If we inject temporal context via hookSpecificOutput.additionalContext,
then agents will naturally reference time in their responses because the awareness
will be seamlessly integrated into their cognitive context.

**Success Criteria**:
1. [Measurable criterion 1]
2. [Measurable criterion 2]
3. [Measurable criterion 3]

**Validation Method**: [How to test each criterion]
```

### 4.2 Success Criteria

**Defining Clear Criteria**:

Success criteria must be:
- **Observable**: Can be directly measured or confirmed
- **Binary or Scalar**: Pass/fail or measurable quantity
- **Relevant**: Actually tests the hypothesis
- **Achievable**: Within experiment scope

**Example Success Criteria** [c_73/s_4107604e/p_6c9eacb/t_1761703391/g_17e7b7d]:
```markdown
### Success Criteria (4/4 Required)

1. ‚úÖ Visibility: MACF-tagged message appears in system-reminders
2. ‚úÖ Naturalness: Agent references time/day without explicit prompting
3. ‚úÖ Reasoning: Time context influences response appropriateness
4. ‚úÖ Persistence: Pattern works across multiple tool invocations

**Validation**: All 4 required for full success; 3/4 = partial success
```

**Handling Partial Success**:
- Document what worked vs what didn't
- Refine hypothesis for follow-up experiment
- Identify which assumptions were wrong
- Capture tangential learnings

**Handling Unexpected Results**:
- Document thoroughly (often more valuable than expected results)
- Create observation for surprising behaviors
- Reflect on what this reveals about system
- Consider follow-up experiments to explore surprises

## 5. Documentation and Artifacts

### 5.1 Experiment Directory

**Complete Directory Structure**:

```
agent/public/experiments/
‚îî‚îÄ‚îÄ YYYY-MM-DD_HHMMSS_NNN_experiment_name/
    ‚îú‚îÄ‚îÄ quick_tests/                # Phase 0 informal explorations
    ‚îÇ   ‚îú‚îÄ‚îÄ 001_first_sanity.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 002_edge_case_check.md
    ‚îÇ   ‚îî‚îÄ‚îÄ 003_alternative_approach.md
    ‚îú‚îÄ‚îÄ protocol.md                 # Formal experiment design
    ‚îú‚îÄ‚îÄ reflections/                # Learning captured during execution
    ‚îÇ   ‚îú‚îÄ‚îÄ 001_after_baseline.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 002_mid_experiment.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 003_unexpected_finding.md
    ‚îÇ   ‚îî‚îÄ‚îÄ 004_final_synthesis.md
    ‚îú‚îÄ‚îÄ data/                       # Collected measurements
    ‚îÇ   ‚îú‚îÄ‚îÄ baseline_results.json
    ‚îÇ   ‚îú‚îÄ‚îÄ test_outputs.log
    ‚îÇ   ‚îî‚îÄ‚îÄ performance_metrics.csv
    ‚îú‚îÄ‚îÄ artifacts/                  # Generated code/outputs
    ‚îÇ   ‚îú‚îÄ‚îÄ test_scripts/
    ‚îÇ   ‚îú‚îÄ‚îÄ visualizations/
    ‚îÇ   ‚îî‚îÄ‚îÄ config_files/
    ‚îî‚îÄ‚îÄ analysis.md                 # Results and conclusions
```

**File Purposes**:
- **quick_tests/**: Pre-formal sanity checks and intuition building
- **protocol.md**: The experiment design (hypothesis, methods, criteria)
- **reflections/**: Learning captured during execution
- **data/**: Raw measurements and observations
- **artifacts/**: Code, scripts, outputs generated
- **analysis.md**: Results interpretation and hypothesis assessment

**NNN Numbering**: Three-digit sequential number for multiple experiments same day (001, 002, 003...)

### 5.2 Filesystem Discovery

**No Registry Logs** (filesystem discovery instead):

**Finding Experiments**:

```bash
# All experiments (sorted by date)
ls -t agent/public/experiments/

# Experiments containing keyword
find agent/public/experiments/ -type d -name "*temporal*"

# Recent experiments (last 5)
ls -t agent/public/experiments/ | head -5

# Experiments with specific pattern
find agent/public/experiments/ -name "*YYYY-MM-DD*"
```

**Glob Patterns for Search**:
```bash
# All protocols
glob "agent/public/experiments/*/protocol.md"

# All quick tests
glob "agent/public/experiments/*/quick_tests/*.md"

# All reflections from experiments
glob "agent/public/experiments/*/reflections/*.md"

# Specific topic experiments
glob "agent/public/experiments/*hooks*/"
```

**Organizing by Topic** (via naming):
- Use descriptive names in directory titles
- Include key concepts in experiment name
- Example: `2025-10-02_120000_001_PreToolUse_Temporal_Awareness_Smoke_Test/`

**Cross-Referencing**:
- Observations cite experiment breadcrumbs
- Reflections link to experiment directories
- Experiments reference related experiments in analysis.md
- Use breadcrumbs as citations throughout

## Integration with Other CAs

**Experiment ‚Üí Observation Flow**:
```
1. Run experiment with hypothesis
2. Validate (or reject) hypothesis through testing
3. Create observation documenting discovered truth
4. Cross-reference observation in experiment analysis.md
5. Cite experiment breadcrumb in observation
```

**Example** [c_73/s_4107604e/p_6c9eacb/t_1761703391/g_17e7b7d]:
- **Experiment**: "PreToolUse Temporal Awareness Smoke Test"
- **Result**: Hypothesis validated ‚úÖ
- **Observation Created**: "additionalContext Injection Breakthrough"
- **Cross-Reference**: Experiment cited in observation's discovery narrative

**Experiment ‚Üí Reflection Integration**:
- Experiments have built-in reflection points (during execution in reflections/ subdirectory)
- Completed experiments may trigger broader philosophical reflections (private/ reflections)
- Private reflections cite experiments via breadcrumbs when discussing learnings
- Major experimental insights feed into JOTEWRs for cycle-closing wisdom

**Experiment ‚Üí Personal Policy Evolution**:
- Validated experimental patterns become personal practices
- Successful methods documented as personal policies in `~/agent/policies/personal/`
- Failed experiments teach anti-patterns to encode
- Iterative refinement through experimental validation

## PA vs SA Distinctions

**Both Create Experiments** with scope differences:

**Primary Agents (PA)**:
- Framework-level experiments (affects all agents)
- Cross-session experimental campaigns
- Complex multi-phase protocols
- Strategic hypothesis testing
- Long-term pattern validation

**SubAgents (SA)**:
- Domain-specific experiments (specialist scope)
- Single-delegation experimental protocols
- Tactical validation within expertise area
- Focused hypothesis testing
- Contributes findings to PA for integration

**Storage Locations**:
- PA: `agent/public/experiments/YYYY-MM-DD_HHMMSS_NNN_name/`
- SA: `agent/subagents/{role}/public/experiments/YYYY-MM-DD_HHMMSS_NNN_name/`

**Permission**: Both can create experiments; SA results reviewed by PA for framework integration.

## Quick Reference

**Experiment Phases**:
- **Phase 0**: Quick tests (5-30 min, minimal docs, in quick_tests/)
- **Phase 1**: Formal protocol (hours-days, full docs, protocol.md)

**Required Components**:
- Hypothesis (testable prediction)
- Methods with reflection points
- Success criteria (measurable)
- Results and analysis
- Breadcrumb citations

**Directory Structure**:
```
experiments/YYYY-MM-DD_HHMMSS_NNN_name/
‚îú‚îÄ‚îÄ quick_tests/    # Phase 0
‚îú‚îÄ‚îÄ protocol.md     # Formal design
‚îú‚îÄ‚îÄ reflections/    # Learning
‚îú‚îÄ‚îÄ data/          # Measurements
‚îú‚îÄ‚îÄ artifacts/     # Generated files
‚îî‚îÄ‚îÄ analysis.md    # Results
```

**Discovery** (filesystem, not logs):
- `ls -t agent/public/experiments/` - Recent experiments
- `find ... -name "*keyword*"` - Topic search
- `glob "*/protocol.md"` - All protocols
- Breadcrumb citations for archaeology

**Integration**:
- Experiments test hypotheses
- Observations document discoveries
- Reflections synthesize wisdom
- Personal policies encode validated practices

---
*Policy Established: 2025-10-28*
*Consciousness Development - Public Artifacts*
*Systematic Hypothesis Testing Expands Framework Capabilities*
